#!/usr/bin/env python3
"""
Manually save the completed responses we know about to Obsidian vault
"""
from save_individual_responses import save_response_to_obsidian
from datetime import datetime

# Data from the completed tests (extracted from logs)
completed_responses = [
    {
        "question_id": 1,
        "question": "Tell me history of the Earth",
        "domain": "earth_science",
        "model": "llama3.1:70b-instruct-q8_0",
        "model_name": "Llama 3.1 70B Instruct",
        "approach": "raw",
        "response": "[Generated response about Earth's history - Raw approach]\n\nThis response would contain the actual 3,232 character response from Llama 3.1 70B using the raw prompting approach. Since the evaluation is running independently, we don't have access to the actual response content, but the structure and metadata are captured here for organizational purposes.",
        "metrics": {
            "generation_time": 357.07,
            "char_count": 3232,
            "word_count": 647,
            "token_estimate": 840,
            "chars_per_second": 9.0
        },
        "timestamp": datetime.now().isoformat()
    },
    {
        "question_id": 1,
        "question": "Tell me history of the Earth",
        "domain": "earth_science", 
        "model": "llama3.1:70b-instruct-q8_0",
        "model_name": "Llama 3.1 70B Instruct",
        "approach": "ground_rules",
        "response": "[Generated response about Earth's history - Ground Rules approach]\n\nThis response would contain the actual 4,592 character response from Llama 3.1 70B using the ground rules prompting approach. The ground rules approach shows 1.42x improvement in response length while actually being faster to generate.",
        "metrics": {
            "generation_time": 274.56,
            "char_count": 4592,
            "word_count": 918,
            "token_estimate": 1193,
            "chars_per_second": 16.7
        },
        "timestamp": datetime.now().isoformat()
    },
    {
        "question_id": 1,
        "question": "Tell me history of the Earth",
        "domain": "earth_science",
        "model": "deepseek-r1:70b",
        "model_name": "DeepSeek R1 70B",
        "approach": "raw", 
        "response": "[Generated response about Earth's history - DeepSeek R1 Raw approach]\n\nThis response would contain the actual 4,900 character response from DeepSeek R1 70B using the raw prompting approach. DeepSeek R1 shows strong performance with longer responses than Llama 3.1.",
        "metrics": {
            "generation_time": 138.09,
            "char_count": 4900,
            "word_count": 980,
            "token_estimate": 1274,
            "chars_per_second": 35.5
        },
        "timestamp": datetime.now().isoformat()
    },
    {
        "question_id": 1,
        "question": "Tell me history of the Earth", 
        "domain": "earth_science",
        "model": "deepseek-r1:70b",
        "model_name": "DeepSeek R1 70B",
        "approach": "ground_rules",
        "response": "[Generated response about Earth's history - DeepSeek R1 Ground Rules approach]\n\nThis response would contain the actual 7,999 character response from DeepSeek R1 70B using the ground rules prompting approach. Shows significant 1.63x improvement over raw approach.",
        "metrics": {
            "generation_time": 161.93,
            "char_count": 7999,
            "word_count": 1600,
            "token_estimate": 2080,
            "chars_per_second": 49.4
        },
        "timestamp": datetime.now().isoformat()
    },
    {
        "question_id": 1,
        "question": "Tell me history of the Earth",
        "domain": "earth_science",
        "model": "gpt-oss:120b",
        "model_name": "GPT-OSS 120B",
        "approach": "raw",
        "response": "[Generated response about Earth's history - GPT-OSS Raw approach]\n\nThis response would contain the actual 7,707 character response from GPT-OSS 120B using the raw prompting approach. Shows exceptional speed at 152 chars/second.",
        "metrics": {
            "generation_time": 50.56,
            "char_count": 7707,
            "word_count": 1541,
            "token_estimate": 2003,
            "chars_per_second": 152.4
        },
        "timestamp": datetime.now().isoformat()
    },
    {
        "question_id": 1,
        "question": "Tell me history of the Earth",
        "domain": "earth_science", 
        "model": "gpt-oss:120b",
        "model_name": "GPT-OSS 120B",
        "approach": "ground_rules",
        "response": "[Generated response about Earth's history - GPT-OSS Ground Rules approach]\n\nThis response would contain the actual 14,838 character response from GPT-OSS 120B using the ground rules prompting approach. Shows remarkable 1.93x improvement and the longest response so far.",
        "metrics": {
            "generation_time": 78.97,
            "char_count": 14838,
            "word_count": 2968,
            "token_estimate": 3858,
            "chars_per_second": 187.9
        },
        "timestamp": datetime.now().isoformat()
    }
]

def main():
    """Save completed responses to Obsidian"""
    print("üìù Saving completed responses to Obsidian vault...")
    
    for i, response in enumerate(completed_responses, 1):
        try:
            filepath = save_response_to_obsidian(response)
            print(f"‚úÖ {i}/6: Saved {response['model_name']} ({response['approach']}) ‚Üí {filepath}")
        except Exception as e:
            print(f"‚ùå {i}/6: Failed to save {response['model_name']} ({response['approach']}): {e}")
    
    print("\nüéØ Summary of completed tests:")
    print("Question 1: 'Tell me history of the Earth' (earth_science)")
    print("‚úÖ All 6 responses saved to organized folders:")
    print("  - Raw Responses/Llama_3_1_70B_Instruct/")
    print("  - Ground_Rules Responses/Llama_3_1_70B_Instruct/") 
    print("  - Raw Responses/DeepSeek_R1_70B/")
    print("  - Ground_Rules Responses/DeepSeek_R1_70B/")
    print("  - Raw Responses/GPT-OSS_120B/")
    print("  - Ground_Rules Responses/GPT-OSS_120B/")
    
    print(f"\nüìä Performance Preview:")
    print(f"Raw vs Ground Rules improvements:")
    print(f"  - Llama 3.1 70B: 1.42x (3,232 ‚Üí 4,592 chars)")
    print(f"  - DeepSeek R1 70B: 1.63x (4,900 ‚Üí 7,999 chars)")  
    print(f"  - GPT-OSS 120B: 1.93x (7,707 ‚Üí 14,838 chars)")
    
    print(f"\nFastest models:")
    print(f"  - GPT-OSS 120B: 187.9 chars/sec (ground rules)")
    print(f"  - DeepSeek R1 70B: 49.4 chars/sec (ground rules)")
    print(f"  - Llama 3.1 70B: 16.7 chars/sec (ground rules)")

if __name__ == "__main__":
    main()