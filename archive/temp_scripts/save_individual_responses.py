#!/usr/bin/env python3
"""
Save individual model responses to Obsidian vault with organized folder structure
"""
from pathlib import Path
from datetime import datetime
from typing import Dict

def save_response_to_obsidian(result: Dict) -> str:
    """Save individual response to organized Obsidian folder structure"""
    
    # Base Obsidian vault path
    obsidian_vault = Path("/Users/tld/Documents/Obsidian LLM")
    educational_folder = obsidian_vault / "Educational"
    
    # Create organized folder structure
    eval_base = educational_folder / "Model Evaluations"
    
    # Organize by approach (Raw vs Ground Rules)
    approach_folder = eval_base / f"{result['approach'].title()} Responses"
    
    # Organize by model
    model_clean = result['model_name'].replace(" ", "_").replace(".", "_")
    model_folder = approach_folder / model_clean
    
    # Create all folders
    model_folder.mkdir(parents=True, exist_ok=True)
    
    # Create filename
    question_id = result.get('question_id', 'unknown')
    domain = result.get('domain', 'general')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    filename = f"Q{question_id}_{domain}_{timestamp}.md"
    filepath = model_folder / filename
    
    # Format the response as Obsidian note
    content = format_response_for_obsidian(result)
    
    # Save to file
    filepath.write_text(content, encoding='utf-8')
    
    print(f"üìù Saved response: {filepath}")
    return str(filepath)

def format_response_for_obsidian(result: Dict) -> str:
    """Format model response as proper Obsidian markdown note"""
    
    question = result.get('question', 'Unknown question')
    domain = result.get('domain', 'general')
    model_name = result.get('model_name', 'Unknown model')
    approach = result.get('approach', 'unknown')
    response = result.get('response', 'No response available')
    
    # Get metrics
    metrics = result.get('metrics', {})
    char_count = metrics.get('char_count', 0)
    word_count = metrics.get('word_count', 0)
    generation_time = metrics.get('generation_time', 0)
    timestamp = result.get('timestamp', datetime.now().isoformat())
    
    # Format content
    content = f"""# {model_name} - {approach.title()} Response

## Question Details
- **Question ID:** {result.get('question_id', 'N/A')}
- **Domain:** {domain}
- **Approach:** {approach.title()}
- **Question:** {question}

## Response Metrics
- **Length:** {char_count:,} characters, {word_count:,} words
- **Generation Time:** {generation_time:.1f} seconds
- **Speed:** {char_count/generation_time:.1f} chars/second
- **Generated:** {timestamp}

## Model Response

{response}

---

**Tags:** #{domain} #{"_".join(model_name.lower().split())} #{approach}
**Generated by:** Educational LLM Evaluation Pipeline
"""
    
    return content

def create_folder_index_files(obsidian_vault_path: str):
    """Create index files for each folder to help with navigation"""
    
    vault = Path(obsidian_vault_path)
    eval_base = vault / "Educational" / "Model Evaluations"
    
    # Create main index
    main_index = eval_base / "README.md"
    main_content = """# Model Evaluations Index

This folder contains comprehensive evaluations of large language models using different prompting approaches.

## Folder Structure

### Raw Responses
Direct model outputs without educational formatting:
- [[Raw Responses/Llama_3_1_70B_Instruct/|Llama 3.1 70B Instruct]]
- [[Raw Responses/DeepSeek_R1_70B/|DeepSeek R1 70B]]
- [[Raw Responses/GPT-OSS_120B/|GPT-OSS 120B]]

### Ground Rules Responses  
Responses using research-focused ground rules prompting:
- [[Ground_Rules Responses/Llama_3_1_70B_Instruct/|Llama 3.1 70B Instruct]]
- [[Ground_Rules Responses/DeepSeek_R1_70B/|DeepSeek R1 70B]]
- [[Ground_Rules Responses/GPT-OSS_120B/|GPT-OSS 120B]]

## Evaluation Questions

1. **Earth Science:** History of the Earth
2. **Scientific Method:** Causality model for scientific discovery
3. **Materials Science:** Hydrogel technology and architecture
4. **Quantum Computing:** Math and AI applications
5. **Numerical Methods:** Step-by-step numerical computation
6. **Machine Learning:** No Free Lunch theorem
7. **Deep Learning:** Optimization methods for Neural Networks
8. **Space Exploration:** Progress and challenges of Mars missions
9. **Space Exploration:** Human space travel plans
10. **Futurism:** Civilization levels
11. **Mathematics:** Multivariable Calculus in model learning
12. **Mathematics:** Information Theory in model performance

## Analysis

- **Comparative Analysis:** See individual responses to compare raw vs ground rules approaches
- **Performance Metrics:** Each response includes generation time and length statistics
- **Domain Insights:** Responses organized by subject domain for domain-specific analysis

---

*Generated by Educational LLM Evaluation Pipeline*
"""
    
    eval_base.mkdir(parents=True, exist_ok=True)
    main_index.write_text(main_content, encoding='utf-8')
    print(f"üìÅ Created main index: {main_index}")

def main():
    """Test the individual response saving"""
    # Create folder structure
    obsidian_path = "/Users/tld/Documents/Obsidian LLM"
    create_folder_index_files(obsidian_path)
    print("‚úÖ Folder structure and indexes created!")

if __name__ == "__main__":
    main()